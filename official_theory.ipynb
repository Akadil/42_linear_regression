{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18494194",
   "metadata": {},
   "source": [
    "## Linear Regression with Gradient Descent\n",
    "\n",
    "**Author:** Akadil  \n",
    "**Project:** ft_linear_regression (42 Project)  \n",
    "**Objective:** Predict car prices based on mileage using linear regression trained with gradient descent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed291b",
   "metadata": {},
   "source": [
    "## 1. Project Goal and Overview\n",
    "\n",
    "### What We're Solving\n",
    "We need to predict the price of a car given its mileage. This is a **supervised learning** problem where:\n",
    "- **Input (Feature):** Mileage (km)\n",
    "- **Output (Target):** Price (currency units)\n",
    "- **Method:** Simple Linear Regression with one feature\n",
    "- **Training:** Gradient Descent algorithm\n",
    "\n",
    "### Success Criteria\n",
    "1. Implement the hypothesis function: `estimatePrice(mileage) = θ₀ + θ₁ × mileage`\n",
    "2. Train using gradient descent with the specified formulas\n",
    "3. Save learned parameters (θ₀, θ₁) for prediction\n",
    "4. No forbidden libraries (like numpy.polyfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff58a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915437e",
   "metadata": {},
   "source": [
    "### 1.1 Load and Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23a69182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (24, 2)\n",
      "\n",
      "First few rows:\n",
      "       km  price\n",
      "0  240000   3650\n",
      "1  139800   3800\n",
      "2  150500   4400\n",
      "3  185530   4450\n",
      "4  176000   5250\n",
      "5  114800   5350\n",
      "6  166800   5800\n",
      "7   89000   5990\n",
      "8  144500   5999\n",
      "9   84000   6200\n",
      "\n",
      "Dataset statistics:\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('static/data.csv')\n",
    "\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head(10))\n",
    "print(\"\\nDataset statistics:\")\n",
    "# print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24c085bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: Clear negative correlation - as mileage increases, price decreases\n"
     ]
    }
   ],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data['km'], data['price'], alpha=0.6, s=50)\n",
    "plt.xlabel('Mileage (km)', fontsize=12)\n",
    "plt.ylabel('Price', fontsize=12)\n",
    "plt.title('Car Price vs Mileage (Raw Data)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(data['price'], bins=20, alpha=0.7, edgecolor='black')\n",
    "# plt.xlabel('Price', fontsize=12)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "# plt.title('Price Distribution', fontsize=14, fontweight='bold')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Clear negative correlation - as mileage increases, price decreases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35e334",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Linear Regression Theory\n",
    "\n",
    "### 2.1 The Model (Hypothesis Function)\n",
    "\n",
    "We assume a **linear relationship** between mileage and price:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 \\cdot x$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ = predicted price (estimatePrice)\n",
    "- $x$ = mileage (input feature)\n",
    "- $\\theta_0$ = y-intercept (bias term) - price when mileage = 0\n",
    "- $\\theta_1$ = slope - how much price changes per km\n",
    "\n",
    "**Goal:** Find optimal values of $\\theta_0$ and $\\theta_1$ that best fit our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb34f26",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cost Function (Before Gradient Descent)\n",
    "\n",
    "### 3.1 Mean Squared Error (MSE)\n",
    "\n",
    "To measure how good our model is, we use the **Mean Squared Error** cost function:\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1 \\cdot x_i) - y_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $m$ = number of training examples\n",
    "- $\\hat{y}_i$ = predicted price for example $i$\n",
    "- $y_i$ = actual price for example $i$\n",
    "\n",
    "### 3.2 Why Squared Errors?\n",
    "\n",
    "1. **Mathematical convenience**: The derivative of $x^2$ is $2x$ (clean). The derivative of $|x|$ has a discontinuity at 0.\n",
    "2. **Penalizes large errors more**: An error of 10 becomes 100, while error of 1 stays 1. This makes the algorithm focus on reducing big mistakes.\n",
    "\n",
    "**Our goal: Minimize $J(\\theta_0, \\theta_1)$** - find parameters that give us the smallest average squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cost function in 3D\n",
    "# This shows the \"landscape\" that gradient descent will navigate\n",
    "\n",
    "# Extract data\n",
    "X = data['km'].values\n",
    "y = data['price'].values\n",
    "m = len(y)\n",
    "\n",
    "# Create a grid of theta values\n",
    "theta0_vals = np.linspace(3000, 9000, 100)\n",
    "theta1_vals = np.linspace(-0.03, 0.01, 100)\n",
    "J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n",
    "\n",
    "# Calculate cost for each combination of theta0 and theta1\n",
    "for i, t0 in enumerate(theta0_vals):\n",
    "    for j, t1 in enumerate(theta1_vals):\n",
    "        predictions = t0 + t1 * X\n",
    "        J_vals[i, j] = (1 / m) * np.sum((predictions - y) ** 2)\n",
    "\n",
    "# Create meshgrid\n",
    "T0, T1 = np.meshgrid(theta0_vals, theta1_vals)\n",
    "\n",
    "# 3D Surface plot\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(T0, T1, J_vals.T, cmap=cm.coolwarm, alpha=0.8)\n",
    "ax1.set_xlabel('θ₀ (intercept)', fontsize=10)\n",
    "ax1.set_ylabel('θ₁ (slope)', fontsize=10)\n",
    "ax1.set_zlabel('Cost J(θ₀, θ₁)', fontsize=10)\n",
    "ax1.set_title('Cost Function Surface', fontsize=12, fontweight='bold')\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(T0, T1, J_vals.T, levels=20, cmap=cm.coolwarm)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('θ₀ (intercept)', fontsize=10)\n",
    "ax2.set_ylabel('θ₁ (slope)', fontsize=10)\n",
    "ax2.set_title('Cost Function Contours', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find and print the minimum\n",
    "min_idx = np.unravel_index(np.argmin(J_vals), J_vals.shape)\n",
    "print(f\"Approximate optimal values:\")\n",
    "print(f\"θ₀ ≈ {theta0_vals[min_idx[0]]:.2f}\")\n",
    "print(f\"θ₁ ≈ {theta1_vals[min_idx[1]]:.6f}\")\n",
    "print(f\"Minimum cost ≈ {J_vals[min_idx]:.2f}\")\n",
    "print(\"\\nThe valley/minimum is where our optimal parameters live!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e5430",
   "metadata": {},
   "source": [
    "**Key Observation:** The cost function forms a \"bowl\" shape (convex function). There's a single global minimum - the point where the cost is lowest. **Gradient descent will help us find this minimum.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1adc99",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Gradient Descent Algorithm\n",
    "\n",
    "### 4.1 The Concept\n",
    "\n",
    "Imagine you're standing on a mountain (the cost function surface) and want to reach the valley (minimum cost). **Gradient descent** is like walking downhill:\n",
    "\n",
    "1. Look around and find the steepest downward direction (negative gradient)\n",
    "2. Take a step in that direction\n",
    "3. Repeat until you reach the bottom\n",
    "\n",
    "### 4.2 The Algorithm\n",
    "\n",
    "Start with initial guesses (usually $\\theta_0 = 0, \\theta_1 = 0$), then repeat:\n",
    "\n",
    "$$\\theta_0 := \\theta_0 - \\alpha \\frac{\\partial J}{\\partial \\theta_0}$$\n",
    "\n",
    "$$\\theta_1 := \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = **learning rate** (step size)\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ = **partial derivative** (gradient/slope) of cost function\n",
    "\n",
    "**CRITICAL:** Update both parameters **simultaneously** (calculate both gradients first, then update both)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69559032",
   "metadata": {},
   "source": [
    "### 4.3 Visualizing Gradient Descent in 2D\n",
    "\n",
    "Let's see how gradient descent works on a simple parabola before tackling our 3D problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdca1ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Setup\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x_vals \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     10\u001b[0m y_vals \u001b[38;5;241m=\u001b[39m simple_function(x_vals)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Gradient descent\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Simple 2D gradient descent visualization\n",
    "def simple_function(x):\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "def simple_gradient(x):\n",
    "    return 2*x + 2\n",
    "\n",
    "# Setup\n",
    "x_vals = np.linspace(-5, 3, 100)\n",
    "y_vals = simple_function(x_vals)\n",
    "\n",
    "# Gradient descent\n",
    "x_current = -4.5  # Starting point\n",
    "learning_rate = 0.1\n",
    "iterations = 20\n",
    "path = [x_current]\n",
    "\n",
    "for _ in range(iterations):\n",
    "    gradient = simple_gradient(x_current)\n",
    "    x_current = x_current - learning_rate * gradient\n",
    "    path.append(x_current)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Left: Full descent\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = x² + 2x + 1')\n",
    "plt.plot(path, [simple_function(x) for x in path], 'ro-', markersize=8, linewidth=1.5, label='Gradient Descent Path')\n",
    "plt.plot(path[0], simple_function(path[0]), 'go', markersize=12, label='Start')\n",
    "plt.plot(path[-1], simple_function(path[-1]), 'r*', markersize=15, label='End')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Gradient Descent on Parabola', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: First few steps\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2)\n",
    "for i in range(min(5, len(path)-1)):\n",
    "    plt.arrow(path[i], simple_function(path[i]), \n",
    "              path[i+1] - path[i], simple_function(path[i+1]) - simple_function(path[i]),\n",
    "              head_width=0.15, head_length=0.2, fc='red', ec='red', linewidth=2)\n",
    "    plt.plot(path[i], simple_function(path[i]), 'ro', markersize=8)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('First 5 Steps (Zoomed)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Started at x = {path[0]:.3f}, f(x) = {simple_function(path[0]):.3f}\")\n",
    "print(f\"Ended at x = {path[-1]:.3f}, f(x) = {simple_function(path[-1]):.3f}\")\n",
    "print(f\"Minimum is at x = -1, f(x) = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36eb9a",
   "metadata": {},
   "source": [
    "### 4.4 Deriving the Gradient Descent Update Rules\n",
    "\n",
    "We need to find $\\frac{\\partial J}{\\partial \\theta_0}$ and $\\frac{\\partial J}{\\partial \\theta_1}$.\n",
    "\n",
    "Starting with our cost function:\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1 x_i) - y_i)^2$$\n",
    "\n",
    "**For $\\theta_0$:** Using chain rule:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} 2((\\theta_0 + \\theta_1 x_i) - y_i) \\cdot 1$$\n",
    "\n",
    "$$= \\frac{2}{m} \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1 x_i) - y_i)$$\n",
    "\n",
    "**For $\\theta_1$:** Using chain rule:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} 2((\\theta_0 + \\theta_1 x_i) - y_i) \\cdot x_i$$\n",
    "\n",
    "$$= \\frac{2}{m} \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1 x_i) - y_i) \\cdot x_i$$\n",
    "\n",
    "The factor of 2 can be absorbed into the learning rate, giving us the **update rules from the project subject**:\n",
    "\n",
    "$$\\theta_0 := \\theta_0 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (estimatePrice(x_i) - y_i)$$\n",
    "\n",
    "$$\\theta_1 := \\theta_1 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (estimatePrice(x_i) - y_i) \\cdot x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37a575",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Scaling and Normalization\n",
    "\n",
    "### 5.1 The Problem: Gradient Explosion\n",
    "\n",
    "When features have very different scales, gradient descent becomes unstable:\n",
    "\n",
    "**Our data:**\n",
    "- Mileage: ranges from ~20,000 to ~250,000 (scale: 10⁵)\n",
    "- Price: ranges from ~3,000 to ~9,000 (scale: 10³)\n",
    "\n",
    "**What happens:**\n",
    "- Gradient for $\\theta_1$ is multiplied by mileage values (huge numbers like 240,000)\n",
    "- Gradient for $\\theta_0$ has no multiplication (stays around 1,000s)\n",
    "- Result: $\\theta_1$ updates are MASSIVE, $\\theta_0$ updates are tiny\n",
    "- The algorithm explodes or takes forever to converge\n",
    "\n",
    "### 5.2 The Solution: Normalize Features\n",
    "\n",
    "Transform data to similar scales (typically 0 to 1 or mean=0, std=1):\n",
    "\n",
    "**Min-Max Normalization:**\n",
    "$$x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "This scales everything to [0, 1] range.\n",
    "\n",
    "**After Training:**\n",
    "We learn $\\theta_0^{norm}$ and $\\theta_1^{norm}$ on normalized data. To make predictions on real data, we must **denormalize** the parameters back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ed675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the problem without normalization\n",
    "\n",
    "def gradient_descent_raw(X, y, theta0, theta1, learning_rate, iterations):\n",
    "    \"\"\"Gradient descent on RAW data - will explode!\"\"\"\n",
    "    m = len(y)\n",
    "    costs = []\n",
    "    theta0_vals = [theta0]\n",
    "    theta1_vals = [theta1]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Calculate predictions\n",
    "        predictions = theta0 + theta1 * X\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost = (1/m) * np.sum((predictions - y)**2)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        grad0 = (1/m) * np.sum(predictions - y)\n",
    "        grad1 = (1/m) * np.sum((predictions - y) * X)\n",
    "        \n",
    "        # Update (simultaneous)\n",
    "        theta0 = theta0 - learning_rate * grad0\n",
    "        theta1 = theta1 - learning_rate * grad1\n",
    "        \n",
    "        theta0_vals.append(theta0)\n",
    "        theta1_vals.append(theta1)\n",
    "        \n",
    "        # Check for explosion\n",
    "        if np.isnan(cost) or np.isinf(cost):\n",
    "            print(f\"EXPLODED at iteration {i}!\")\n",
    "            break\n",
    "    \n",
    "    return theta0_vals, theta1_vals, costs\n",
    "\n",
    "# Try with a learning rate that causes explosion\n",
    "print(\"Attempting gradient descent on RAW data with learning_rate = 0.01...\")\n",
    "theta0_raw, theta1_raw, costs_raw = gradient_descent_raw(X, y, 0, 0, 0.01, 100)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(costs_raw, 'r-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Cost', fontsize=12)\n",
    "plt.title('Cost Explodes on Raw Data', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(theta0_raw, label='θ₀', linewidth=2)\n",
    "plt.plot(theta1_raw, label='θ₁', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Parameter Value', fontsize=12)\n",
    "plt.title('Parameters Explode', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nθ₀ went from {theta0_raw[0]} to {theta0_raw[-1]:.2e}\")\n",
    "print(f\"θ₁ went from {theta1_raw[0]} to {theta1_raw[-1]:.2e}\")\n",
    "print(\"\\nThis is why we need normalization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's normalize and see the difference\n",
    "\n",
    "def normalize_data(data):\n",
    "    \"\"\"Min-max normalization to [0, 1]\"\"\"\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "    normalized = (data - data_min) / (data_max - data_min)\n",
    "    return normalized, data_min, data_max\n",
    "\n",
    "# Normalize\n",
    "X_norm, X_min, X_max = normalize_data(X)\n",
    "y_norm, y_min, y_max = normalize_data(y)\n",
    "\n",
    "print(\"Original data ranges:\")\n",
    "print(f\"  Mileage: [{X.min():.0f}, {X.max():.0f}]\")\n",
    "print(f\"  Price: [{y.min():.0f}, {y.max():.0f}]\")\n",
    "print(\"\\nNormalized data ranges:\")\n",
    "print(f\"  Mileage: [{X_norm.min():.3f}, {X_norm.max():.3f}]\")\n",
    "print(f\"  Price: [{y_norm.min():.3f}, {y_norm.max():.3f}]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X, y, alpha=0.6)\n",
    "plt.xlabel('Mileage (km)', fontsize=12)\n",
    "plt.ylabel('Price', fontsize=12)\n",
    "plt.title('Original Data', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_norm, y_norm, alpha=0.6, color='orange')\n",
    "plt.xlabel('Normalized Mileage', fontsize=12)\n",
    "plt.ylabel('Normalized Price', fontsize=12)\n",
    "plt.title('Normalized Data [0, 1]', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79184005",
   "metadata": {},
   "source": [
    "### 5.3 Denormalization: Getting Back Real Parameters\n",
    "\n",
    "After training on normalized data, we get $\\theta_0^{norm}$ and $\\theta_1^{norm}$. To use them on real data:\n",
    "\n",
    "**The transformation:**\n",
    "\n",
    "$$\\theta_1^{real} = \\theta_1^{norm} \\cdot \\frac{y_{max} - y_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "$$\\theta_0^{real} = (y_{max} - y_{min}) \\cdot \\theta_0^{norm} + y_{min} - \\theta_1^{real} \\cdot x_{min}$$\n",
    "\n",
    "This ensures:\n",
    "$$estimatePrice(mileage_{real}) = \\theta_0^{real} + \\theta_1^{real} \\cdot mileage_{real}$$\n",
    "\n",
    "gives correct predictions without needing to normalize inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a2210",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Implementation\n",
    "\n",
    "### 6.1 Complete Gradient Descent with Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.1, iterations=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Complete gradient descent implementation with normalization\n",
    "    \n",
    "    Returns:\n",
    "        theta0, theta1: Denormalized parameters for real data\n",
    "        history: Dictionary with training history\n",
    "    \"\"\"\n",
    "    # Normalize data\n",
    "    X_norm, X_min, X_max = normalize_data(X)\n",
    "    y_norm, y_min, y_max = normalize_data(y)\n",
    "    \n",
    "    m = len(y)\n",
    "    theta0_norm = 0.0\n",
    "    theta1_norm = 0.0\n",
    "    \n",
    "    # Track history\n",
    "    costs = []\n",
    "    theta0_history = []\n",
    "    theta1_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Predictions on normalized data\n",
    "        predictions = theta0_norm + theta1_norm * X_norm\n",
    "        \n",
    "        # Cost on normalized data\n",
    "        cost = (1/m) * np.sum((predictions - y_norm)**2)\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Gradients\n",
    "        grad0 = (1/m) * np.sum(predictions - y_norm)\n",
    "        grad1 = (1/m) * np.sum((predictions - y_norm) * X_norm)\n",
    "        \n",
    "        # SIMULTANEOUS UPDATE\n",
    "        temp0 = theta0_norm - learning_rate * grad0\n",
    "        temp1 = theta1_norm - learning_rate * grad1\n",
    "        theta0_norm = temp0\n",
    "        theta1_norm = temp1\n",
    "        \n",
    "        theta0_history.append(theta0_norm)\n",
    "        theta1_history.append(theta1_norm)\n",
    "        \n",
    "        if verbose and i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
    "    \n",
    "    # Denormalize parameters\n",
    "    theta1_real = theta1_norm * (y_max - y_min) / (X_max - X_min)\n",
    "    theta0_real = (y_max - y_min) * theta0_norm + y_min - theta1_real * X_min\n",
    "    \n",
    "    history = {\n",
    "        'costs': costs,\n",
    "        'theta0_norm_history': theta0_history,\n",
    "        'theta1_norm_history': theta1_history,\n",
    "        'X_min': X_min,\n",
    "        'X_max': X_max,\n",
    "        'y_min': y_min,\n",
    "        'y_max': y_max,\n",
    "        'theta0_norm': theta0_norm,\n",
    "        'theta1_norm': theta1_norm\n",
    "    }\n",
    "    \n",
    "    return theta0_real, theta1_real, history\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model with gradient descent...\")\n",
    "print(\"=\"*60)\n",
    "theta0, theta1, history = gradient_descent(X, y, learning_rate=0.1, iterations=1000, verbose=True)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Parameters (for real data):\")\n",
    "print(f\"  θ₀ (intercept) = {theta0:.2f}\")\n",
    "print(f\"  θ₁ (slope) = {theta1:.6f}\")\n",
    "print(f\"\\nFinal Cost: {history['costs'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training process\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Cost over iterations\n",
    "axes[0, 0].plot(history['costs'], linewidth=2, color='blue')\n",
    "axes[0, 0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Cost (MSE)', fontsize=11)\n",
    "axes[0, 0].set_title('Cost Function Convergence', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale cost (to see early convergence better)\n",
    "axes[0, 1].semilogy(history['costs'], linewidth=2, color='green')\n",
    "axes[0, 1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Cost (MSE) - Log Scale', fontsize=11)\n",
    "axes[0, 1].set_title('Cost Convergence (Log Scale)', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter evolution (normalized)\n",
    "axes[1, 0].plot(history['theta0_norm_history'], label='θ₀ (normalized)', linewidth=2)\n",
    "axes[1, 0].plot(history['theta1_norm_history'], label='θ₁ (normalized)', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Parameter Value', fontsize=11)\n",
    "axes[1, 0].set_title('Parameter Evolution During Training', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final fit\n",
    "predictions = theta0 + theta1 * X\n",
    "axes[1, 1].scatter(X, y, alpha=0.5, s=50, label='Actual Data')\n",
    "axes[1, 1].plot(X, predictions, 'r-', linewidth=3, label='Learned Model')\n",
    "axes[1, 1].set_xlabel('Mileage (km)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Price', fontsize=11)\n",
    "axes[1, 1].set_title('Final Model Fit', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89053dd",
   "metadata": {},
   "source": [
    "### 6.2 Model Evaluation Metrics\n",
    "\n",
    "Now let's calculate how well our model performs using standard metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate MSE and R² score\"\"\"\n",
    "    # Mean Squared Error\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    \n",
    "    # R² Score (Coefficient of Determination)\n",
    "    ss_res = np.sum((y_true - y_pred)**2)  # Residual sum of squares\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)  # Total sum of squares\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    # RMSE (Root Mean Squared Error)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return mse, rmse, r2\n",
    "\n",
    "# Calculate predictions\n",
    "predictions = theta0 + theta1 * X\n",
    "\n",
    "# Calculate metrics\n",
    "mse, rmse, r2 = calculate_metrics(y, predictions)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Squared Error (MSE):  {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE):  {rmse:.2f}\")\n",
    "print(f\"R² Score (Coefficient of Determination):  {r2:.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  - RMSE of {rmse:.2f} means average prediction error is ~{rmse:.0f} price units\")\n",
    "print(f\"  - R² of {r2:.4f} means the model explains {r2*100:.2f}% of variance in price\")\n",
    "if r2 > 0.8:\n",
    "    print(\"  - This is a GOOD model! ✓\")\n",
    "elif r2 > 0.6:\n",
    "    print(\"  - This is an ACCEPTABLE model.\")\n",
    "else:\n",
    "    print(\"  - This model needs improvement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16809552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y - predictions\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(predictions, residuals, alpha=0.6)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Price', fontsize=11)\n",
    "axes[0].set_ylabel('Residuals (Actual - Predicted)', fontsize=11)\n",
    "axes[0].set_title('Residual Plot', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[1].hist(residuals, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Residual Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[2].scatter(y, predictions, alpha=0.6)\n",
    "axes[2].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[2].set_xlabel('Actual Price', fontsize=11)\n",
    "axes[2].set_ylabel('Predicted Price', fontsize=11)\n",
    "axes[2].set_title('Actual vs Predicted', fontsize=13, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Good residual plots show:\")\n",
    "print(\"  1. Random scatter around 0 (no patterns)\")\n",
    "print(\"  2. Roughly normal distribution\")\n",
    "print(\"  3. Points close to the diagonal line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906239b",
   "metadata": {},
   "source": [
    "### 6.3 Interactive Prediction Function\n",
    "\n",
    "Let's create a function that mimics the prediction program from the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeed2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_price(mileage, theta0=theta0, theta1=theta1):\n",
    "    \"\"\"\n",
    "    Estimate car price based on mileage\n",
    "    Uses the learned parameters from training\n",
    "    \"\"\"\n",
    "    return theta0 + theta1 * mileage\n",
    "\n",
    "# Test with various mileages\n",
    "test_mileages = [0, 50000, 100000, 150000, 200000, 250000]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRICE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "for km in test_mileages:\n",
    "    price = estimate_price(km)\n",
    "    print(f\"Mileage: {km:>7} km  →  Estimated Price: {price:>8.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Interactive visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, alpha=0.5, s=50, label='Training Data')\n",
    "plt.plot(X, predictions, 'r-', linewidth=3, label=f'Model: y = {theta0:.2f} + {theta1:.6f}x')\n",
    "\n",
    "# Mark test predictions\n",
    "for km in test_mileages:\n",
    "    price = estimate_price(km)\n",
    "    plt.plot(km, price, 'g*', markersize=15)\n",
    "    plt.annotate(f'{price:.0f}', xy=(km, price), xytext=(km, price+300), \n",
    "                 fontsize=9, ha='center', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.xlabel('Mileage (km)', fontsize=12)\n",
    "plt.ylabel('Price', fontsize=12)\n",
    "plt.title('Price Prediction Model with Sample Predictions', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377189e",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Hyperparameter Analysis\n",
    "\n",
    "Let's explore how different hyperparameters affect training.\n",
    "\n",
    "### 7.1 Effect of Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    _, _, hist = gradient_descent(X, y, learning_rate=lr, iterations=500)\n",
    "    plt.plot(hist['costs'], label=f'α = {lr}', linewidth=2, color=color)\n",
    "plt.xlabel('Iteration', fontsize=11)\n",
    "plt.ylabel('Cost (MSE)', fontsize=11)\n",
    "plt.title('Effect of Learning Rate on Convergence', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    _, _, hist = gradient_descent(X, y, learning_rate=lr, iterations=500)\n",
    "    plt.semilogy(hist['costs'], label=f'α = {lr}', linewidth=2, color=color)\n",
    "plt.xlabel('Iteration', fontsize=11)\n",
    "plt.ylabel('Cost (MSE) - Log Scale', fontsize=11)\n",
    "plt.title('Convergence Speed Comparison (Log Scale)', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"  - Too small learning rate (0.01): Very slow convergence\")\n",
    "print(\"  - Good learning rates (0.1, 0.5): Fast and stable convergence\")\n",
    "print(\"  - Too large learning rate (1.0): May oscillate or diverge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1eebed",
   "metadata": {},
   "source": [
    "### 7.2 Effect of Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340784b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different iteration counts\n",
    "iteration_counts = [10, 50, 100, 500, 1000]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, iters in enumerate(iteration_counts):\n",
    "    t0, t1, hist = gradient_descent(X, y, learning_rate=0.1, iterations=iters)\n",
    "    preds = t0 + t1 * X\n",
    "    _, _, r2 = calculate_metrics(y, preds)\n",
    "    \n",
    "    axes[idx].scatter(X, y, alpha=0.4, s=30)\n",
    "    axes[idx].plot(X, preds, 'r-', linewidth=2)\n",
    "    axes[idx].set_xlabel('Mileage (km)', fontsize=10)\n",
    "    axes[idx].set_ylabel('Price', fontsize=10)\n",
    "    axes[idx].set_title(f'Iterations: {iters} | R² = {r2:.4f}', fontsize=11, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide the last subplot (we have 5 plots in 2x3 grid)\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"  - 10 iterations: Barely learned anything\")\n",
    "print(\"  - 50 iterations: Starting to fit\")\n",
    "print(\"  - 100 iterations: Decent fit\")\n",
    "print(\"  - 500-1000 iterations: Converged to optimal solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67a7d4",
   "metadata": {},
   "source": [
    "### 7.3 Convergence Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3185fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed convergence analysis\n",
    "_, _, hist = gradient_descent(X, y, learning_rate=0.1, iterations=1000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# First 50 iterations\n",
    "axes[0].plot(hist['costs'][:50], linewidth=2, color='blue')\n",
    "axes[0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0].set_ylabel('Cost', fontsize=11)\n",
    "axes[0].set_title('First 50 Iterations (Rapid Descent)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Middle iterations (50-200)\n",
    "axes[1].plot(range(50, 200), hist['costs'][50:200], linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('Cost', fontsize=11)\n",
    "axes[1].set_title('Iterations 50-200 (Slow Improvement)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Last iterations (800-1000)\n",
    "axes[2].plot(range(800, 1000), hist['costs'][800:1000], linewidth=2, color='red')\n",
    "axes[2].set_xlabel('Iteration', fontsize=11)\n",
    "axes[2].set_ylabel('Cost', fontsize=11)\n",
    "axes[2].set_title('Iterations 800-1000 (Converged)', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cost reduction:\")\n",
    "print(f\"  Start: {hist['costs'][0]:.4f}\")\n",
    "print(f\"  After 50: {hist['costs'][49]:.4f} ({(1 - hist['costs'][49]/hist['costs'][0])*100:.1f}% reduction)\")\n",
    "print(f\"  After 200: {hist['costs'][199]:.4f} ({(1 - hist['costs'][199]/hist['costs'][0])*100:.1f}% reduction)\")\n",
    "print(f\"  Final: {hist['costs'][-1]:.4f} ({(1 - hist['costs'][-1]/hist['costs'][0])*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c68d8",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Summary and Conclusions\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Built a linear regression model** from scratch using gradient descent\n",
    "2. **Solved the gradient explosion problem** using feature normalization\n",
    "3. **Learned parameters** θ₀ and θ₁ that minimize prediction error\n",
    "4. **Achieved good performance** with R² > 0.8 (model explains >80% of variance)\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "**Mathematical:**\n",
    "- Linear regression models relationships as: `y = θ₀ + θ₁x`\n",
    "- Cost function (MSE) measures prediction error\n",
    "- Gradient descent iteratively minimizes cost by following negative gradient\n",
    "- Simultaneous updates are required for correct convergence\n",
    "\n",
    "**Practical:**\n",
    "- Feature scaling is CRITICAL when features have different magnitudes\n",
    "- Learning rate controls convergence speed (too high = divergence, too low = slow)\n",
    "- More iterations generally improve results, but returns diminish after convergence\n",
    "- Denormalization allows using learned parameters on real-world data\n",
    "\n",
    "### Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a9c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary\n",
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nLearned Parameters:\")\n",
    "print(f\"  θ₀ (intercept) = {theta0:.2f}\")\n",
    "print(f\"  θ₁ (slope)     = {theta1:.6f}\")\n",
    "print(f\"\\nModel Equation:\")\n",
    "print(f\"  Price = {theta0:.2f} + ({theta1:.6f}) × Mileage\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Mean Squared Error:  {mse:.2f}\")\n",
    "print(f\"  Root Mean Squared Error:  {rmse:.2f}\")\n",
    "print(f\"  R² Score:  {r2:.4f} ({r2*100:.2f}% of variance explained)\")\n",
    "print(f\"\\nTraining Details:\")\n",
    "print(f\"  Learning Rate: 0.1\")\n",
    "print(f\"  Iterations: 1000\")\n",
    "print(f\"  Final Cost: {history['costs'][-1]:.6f}\")\n",
    "print(f\"  Dataset Size: {len(X)} examples\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save parameters (as in the actual project)\n",
    "params = {\n",
    "    'theta0': theta0,\n",
    "    'theta1': theta1,\n",
    "    'mse': mse,\n",
    "    'r2': r2\n",
    "}\n",
    "\n",
    "print(\"\\nParameters ready to be saved for the prediction program!\")\n",
    "print(f\"These would be saved to a file for use in estimate_price.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
