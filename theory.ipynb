{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ea050e",
   "metadata": {},
   "source": [
    "## The theory behind ft_linear_regression project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6af37e",
   "metadata": {},
   "source": [
    "The formula of the linear regression is:\n",
    "$$y(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable (the output we want to predict)\n",
    "- $x$ is the independent variable (the input feature)\n",
    "- $\\theta_0$ is the intercept (the value of $y$ when $x = 0$)\n",
    "- $\\theta_1$ is the slope (the change in $y$ for a one-unit change in $x$)\n",
    "\n",
    "This is the data points we will be working with:\n",
    "<p align=\"center\">\n",
    "  <img src=\"static/data_plot.png\" width=\"50%\">\n",
    "</p>\n",
    "\n",
    "The goal of linear regression is to find the best-fitting line through the data points, which minimizes the difference between the predicted values and the actual values. This is typically done using the method of least squares, which minimizes the sum of the squared differences between the predicted and actual values.\n",
    "\n",
    "The cost function for linear regression is given by:\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Where:\n",
    "- $J(\\theta)$ is the cost function\n",
    "- $m$ is the number of training examples\n",
    "- $h_\\theta(x^{(i)})$ is the predicted value for the $i$-th training example\n",
    "- $y^{(i)}$ is the actual value for the $i$-th training example\n",
    "- $\\theta$ is the vector of parameters (including both $\\theta_0$ and $\\theta_1$)\n",
    "\n",
    "The gradient descent algorithm is used to minimize the cost function by iteratively updating the parameters $\\theta$:\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "Where: \n",
    "- $\\alpha$ is the learning rate (a hyperparameter that controls the step size in the parameter space)\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$ is the partial derivative of the cost function with respect to the parameter $\\theta_j$\n",
    "The partial derivatives for the parameters $\\theta_0$ and $\\theta_1$ are given by:\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})$$\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)}$$\n",
    "\n",
    "--- \n",
    "## Evaluation of the Model\n",
    "\n",
    "The final model can be evaluated using metrics such as Mean Squared Error (MSE) or R-squared, which provide insights into the model's performance and how well it fits the data.\n",
    "The MSE is calculated as:\n",
    "$$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "The R-squared value is calculated as:\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^{m} (y^{(i)} - h_\\theta(x^{(i)}))^2}{\\sum_{i=1}^{m} (y^{(i)} - \\bar{y})^2}$$\n",
    "Where $\\bar{y}$ is the mean of the actual values $y^{(i)}$.\n",
    "The R-squared value indicates the proportion of variance in the dependent variable that can be explained by the independent variable. A higher R-squared value indicates a better fit of the model to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0952cdb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
